{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from collections import defaultdict\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from IPython.core.display import display, HTML\n",
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import words as nltk_words\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import  RandomizedSearchCV\n",
    "from sklearn.preprocessing import Imputer, StandardScaler, OneHotEncoder, LabelBinarizer, FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, SelectPercentile\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for data, acronyms, and gensim paths\n",
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "config.read('../../config.ini')\n",
    "\n",
    "DB_PATH = config['DATABASES']['PROJECT_DB_PATH']\n",
    "AIRLINE_ACRONYMS_FILEPATH = config['NLP']['AIRLINE_ACRONYMS_FILEPATH']\n",
    "AIRLINE_CLEANED_TEXT_PATH = config['NLP']['AIRLINE_CLEANED_TEXT_PATH']\n",
    "GENSIM_DICTIONARY_PATH = config['NLP']['GENSIM_DICTIONARY_PATH']\n",
    "GENSIM_CORPUS_PATH = config['NLP']['GENSIM_CORPUS_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>section_name</th>\n",
       "      <th>section_text</th>\n",
       "      <th>criteria</th>\n",
       "      <th>section_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>292</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>DEPARTMENT OF TRANSPORTATION RANKINGS FOR 1994...</td>\n",
       "      <td>A multitude of challenges faced the People of ...</td>\n",
       "      <td>&lt;, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...</td>\n",
       "      <td>2849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>298</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>RESULTS OF OPERATIONS</td>\n",
       "      <td>1994 COMPARED WITH 1993 The Company's consolid...</td>\n",
       "      <td>&lt;, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...</td>\n",
       "      <td>13806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>306</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>ACQUISITION</td>\n",
       "      <td>On December 31, 1993, Southwest exchanged 3,57...</td>\n",
       "      <td>&lt;, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...</td>\n",
       "      <td>2141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>309</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>ACCRUED LIABILITIES (IN THOUSANDS) LONG-TERM D...</td>\n",
       "      <td>On March 1, 1993, the Company redeemed the $10...</td>\n",
       "      <td>&lt;, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...</td>\n",
       "      <td>1855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>360</td>\n",
       "      <td>southwest-airlines-co_annual_report_1995.docx</td>\n",
       "      <td>SECRET NUMBER 1 STICK TO WHAT YOU’RE GOOD AT.</td>\n",
       "      <td>Since 1971, Southwest Airlines has offered sin...</td>\n",
       "      <td>&lt;, f, u, n, c, t, i, o, n,  , s, t, y, l, e,  ...</td>\n",
       "      <td>2566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     section_id                                       filename  \\\n",
       "291         292  southwest-airlines-co_annual_report_1994.docx   \n",
       "297         298  southwest-airlines-co_annual_report_1994.docx   \n",
       "305         306  southwest-airlines-co_annual_report_1994.docx   \n",
       "308         309  southwest-airlines-co_annual_report_1994.docx   \n",
       "359         360  southwest-airlines-co_annual_report_1995.docx   \n",
       "\n",
       "                                          section_name  \\\n",
       "291  DEPARTMENT OF TRANSPORTATION RANKINGS FOR 1994...   \n",
       "297                              RESULTS OF OPERATIONS   \n",
       "305                                        ACQUISITION   \n",
       "308  ACCRUED LIABILITIES (IN THOUSANDS) LONG-TERM D...   \n",
       "359      SECRET NUMBER 1 STICK TO WHAT YOU’RE GOOD AT.   \n",
       "\n",
       "                                          section_text  \\\n",
       "291  A multitude of challenges faced the People of ...   \n",
       "297  1994 COMPARED WITH 1993 The Company's consolid...   \n",
       "305  On December 31, 1993, Southwest exchanged 3,57...   \n",
       "308  On March 1, 1993, the Company redeemed the $10...   \n",
       "359  Since 1971, Southwest Airlines has offered sin...   \n",
       "\n",
       "                                              criteria  section_length  \n",
       "291  <, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...            2849  \n",
       "297  <, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...           13806  \n",
       "305  <, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...            2141  \n",
       "308  <, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...            1855  \n",
       "359  <, f, u, n, c, t, i, o, n,  , s, t, y, l, e,  ...            2566  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine(DB_PATH)\n",
    "df = pd.read_sql(\"SELECT * FROM Sections\", con=engine)\n",
    "\n",
    "# the annual report from 1992 was scanned in poor quality\n",
    "# and the text was not legible\n",
    "df = df[df.filename != 'southwest-airlines-co_annual_report_1992.docx']\n",
    "\n",
    "# filter to relevant sections\n",
    "df = df[df['section_text'].str.contains('fee')]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A multitude of challenges faced the People of Southwest Airlines in 1994. The mark of a true champion is the ability to “rise to the occasion” and meet challenges. We believe our Employees showed their true Southwest Spirit in 1994, accomplishing three- or four-fold what a normal year would  bring.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store section matches in list\n",
    "text = [section for section in df['section_text'].values]\n",
    "\n",
    "# review first sentence of a section match\n",
    "text[0][0:299]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.columns = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = pd.DataFrame()\n",
    "        df['title_punc_count']= X.apply(lambda sent: sum(1 for word in sent if word in string.punctuation))\n",
    "        df['title_exclamation_count']= X.str.count('!')\n",
    "        df['title_all_caps'] = X.apply(lambda x: x.isupper())\n",
    "        df['contains_numbers'] = X.apply(lambda s: any(i.isdigit() for i in s))\n",
    "        df['sent_len'] = X.apply(len)\n",
    "        df['num_distinct_words'] = X.apply(lambda sent: len(set(sent.split())))\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.columns\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_punc_count</th>\n",
       "      <th>title_exclamation_count</th>\n",
       "      <th>title_all_caps</th>\n",
       "      <th>contains_numbers</th>\n",
       "      <th>sent_len</th>\n",
       "      <th>num_distinct_words</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2849</td>\n",
       "      <td>273</td>\n",
       "      <td>A multitude of challenges faced the People of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>434</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>13806</td>\n",
       "      <td>649</td>\n",
       "      <td>1994 COMPARED WITH 1993 The Company's consolid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2141</td>\n",
       "      <td>172</td>\n",
       "      <td>On December 31, 1993, Southwest exchanged 3,57...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1855</td>\n",
       "      <td>174</td>\n",
       "      <td>On March 1, 1993, the Company redeemed the $10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2566</td>\n",
       "      <td>243</td>\n",
       "      <td>Since 1971, Southwest Airlines has offered sin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     title_punc_count  title_exclamation_count  title_all_caps  \\\n",
       "291                61                        0           False   \n",
       "297               434                        0           False   \n",
       "305                54                        0           False   \n",
       "308                83                        0           False   \n",
       "359                84                        0           False   \n",
       "\n",
       "     contains_numbers  sent_len  num_distinct_words  \\\n",
       "291              True      2849                 273   \n",
       "297              True     13806                 649   \n",
       "305              True      2141                 172   \n",
       "308              True      1855                 174   \n",
       "359              True      2566                 243   \n",
       "\n",
       "                                                  text  \n",
       "291  A multitude of challenges faced the People of ...  \n",
       "297  1994 COMPARED WITH 1993 The Company's consolid...  \n",
       "305  On December 31, 1993, Southwest exchanged 3,57...  \n",
       "308  On March 1, 1993, the Company redeemed the $10...  \n",
       "359  Since 1971, Southwest Airlines has offered sin...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar = GrammarTransformer()\n",
    "grammar.fit(text)\n",
    "result = grammar.transform(df['section_text'])\n",
    "result['text'] = df['section_text']\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" \n",
    "    extract housing features, using dependency parsing to combine descriptive words with their ROOT or noun subject\n",
    "    e.g. 'beautiful brownstone 1 bedroom' ---> 'beautiful bedroom', 'brownstone bedroom', '1 bedroom'\n",
    "    this may improve upon n-grams which would connect words by character location, not meaning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text_field):\n",
    "        self.text_field = text_field\n",
    "        self.columns = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        #time_text = []\n",
    "        #gpe_text = []\n",
    "        gpe_count = []\n",
    "        cardinal_count = []\n",
    "        time_count = []\n",
    "        quantity_count = []\n",
    "\n",
    "        for doc in nlp.pipe(df[self.text_field], disable=['tagger']):\n",
    "            _gpe,_cardinal,_time,_quantity = 0,0,0,0\n",
    "            #_gpe_text,_time_text = '',''\n",
    "\n",
    "            for ent in doc.ents:    \n",
    "                if ent.label_ == 'GPE':\n",
    "                    _gpe += 1\n",
    "                    #_gpe_text = ' '.join([_gpe_text, ent.text])\n",
    "                if ent.label_ == 'CARDINAL':\n",
    "                    _cardinal += 1\n",
    "                if ent.label_ == 'DATE':\n",
    "                    _time += 1\n",
    "                   # _time_text = ' '.join([_time_text, ent.text])\n",
    "                if ent.label_ == 'QUANTITY':\n",
    "                    _quantity += 1\n",
    "\n",
    "            #gpe_text.append(_gpe_text)\n",
    "            #time_text.append(_time_text)  \n",
    "            gpe_count.append(_gpe)\n",
    "            cardinal_count.append(_cardinal)\n",
    "            time_count.append(_time)\n",
    "            quantity_count.append(_quantity)\n",
    "        \n",
    "        df['gpe_count'] = gpe_count\n",
    "        df['cardinal_count'] = cardinal_count\n",
    "        df['time_count'] = time_count\n",
    "        df['quantity_count'] = quantity_count\n",
    "        \n",
    "        df.drop(self.text_field, axis=1, inplace=True)\n",
    "        self.columns = df.columns\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousingTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" \n",
    "    extract housing features, using dependency parsing to combine descriptive words with their ROOT or noun subject\n",
    "    e.g. 'beautiful brownstone 1 bedroom' ---> 'beautiful bedroom', 'brownstone bedroom', '1 bedroom'\n",
    "    this may improve upon n-grams which would connect words by character location, not meaning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text_field):\n",
    "        self.text_field = text_field\n",
    "        self.columns = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        home_size = []\n",
    "        home_type_descriptions = []\n",
    "        home_type = []\n",
    "        home_description_count = []\n",
    "\n",
    "        for doc in nlp.pipe(df[self.text_field], disable=['tagger','ner']):\n",
    "            # containers for housing types and descriptions for each listing\n",
    "            _type = ''\n",
    "            _size = ''\n",
    "            _type_descriptions = ''\n",
    "            _description_count = 0\n",
    "\n",
    "            for token in doc:\n",
    "                # find root and nsubj - most likely to be housing types\n",
    "                if (token.dep_ == 'ROOT') or (token.dep_ == 'nsubj'):\n",
    "                    token_text = token.lemma_.lower()\n",
    "                    _type = ' '.join([_type, token_text])\n",
    "\n",
    "                    # find all words that are preceeded by a num\n",
    "                    for left_term in [t.text for t in token.lefts]:\n",
    "                        if left_term.isdigit():\n",
    "                            _size = ' '.join(\n",
    "                                [_size,'{}_{}'.format(left_term,token_text)])\n",
    "                    for left_term in [t.text for t in token.lefts]:\n",
    "                        _type_descriptions = ' '.join(\n",
    "                            [_type_descriptions,'{}_{}'.format(left_term,token_text)])\n",
    "                        _description_count += 1\n",
    "\n",
    "            home_type.append(_type)\n",
    "            home_size.append(_size)\n",
    "            home_type_descriptions.append(_type_descriptions)\n",
    "            home_description_count.append(_description_count)\n",
    "\n",
    "        # cast list of terms to string for each listing's housing features\n",
    "        df['home_type'] = home_type\n",
    "        df['home_size'] = home_size\n",
    "        df['home_type_descriptions'] = home_type_descriptions\n",
    "        df['home_description_count'] = home_description_count\n",
    "\n",
    "        # combine all housing text features into a single column, space separated\n",
    "        # this single column will be used for text vectorization of housing features\n",
    "        df['_space'] = ' '\n",
    "        df['housing_features'] = df[['home_type','_space'\n",
    "                                       , 'home_size','_space'\n",
    "                                       , 'home_type_descriptions'\n",
    "                                      ]].sum(axis=1)\n",
    "        self.columns = df.columns\n",
    "        \n",
    "        return df['housing_features']\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SOURCE: scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Select a subset of features as a step in a sklearn pipeline \"\"\"\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class DummyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.dummy_cols = None\n",
    "            \n",
    "    def fit(self, X, y=None):\n",
    "        self.dummy_cols = pd.get_dummies(X).columns\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = pd.get_dummies(X)\n",
    "        for col in self.dummy_cols:\n",
    "            if col not in X.columns:\n",
    "                X[col] = 0\n",
    "        \n",
    "        return X[self.dummy_cols]\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.dummy_cols\n",
    "        \n",
    "    \n",
    "class SparseMatrixTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Converts a dense matrix into a sparse matrix\n",
    "\n",
    "    Note: used in sklearn pipeline to reformat numeric data to match sparse matrix for text data\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        train_extra = sp.sparse.csr_matrix(X.astype(float))\n",
    "\n",
    "        return train_extra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:guild]",
   "language": "python",
   "name": "conda-env-guild-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
